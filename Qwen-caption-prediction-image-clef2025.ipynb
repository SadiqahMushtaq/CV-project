{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":11763442,"sourceType":"datasetVersion","datasetId":7385001},{"sourceId":11763821,"sourceType":"datasetVersion","datasetId":7385231}],"dockerImageVersionId":31011,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"try: import torch\nexcept: raise ImportError('Install torch via `pip install torch`')\nfrom packaging.version import Version as V\nv = V(torch.__version__)\ncuda = str(torch.version.cuda)\nis_ampere = torch.cuda.get_device_capability()[0] >= 8\nif cuda != \"12.1\" and cuda != \"11.8\" and cuda != \"12.4\": raise RuntimeError(f\"CUDA = {cuda} not supported!\")\nif   v <= V('2.1.0'): raise RuntimeError(f\"Torch = {v} too old!\")\nelif v <= V('2.1.1'): x = 'cu{}{}-torch211'\nelif v <= V('2.1.2'): x = 'cu{}{}-torch212'\nelif v  < V('2.3.0'): x = 'cu{}{}-torch220'\nelif v  < V('2.4.0'): x = 'cu{}{}-torch230'\nelif v  < V('2.5.0'): x = 'cu{}{}-torch240'\nelif v  < V('2.6.0'): x = 'cu{}{}-torch250'\nelse: raise RuntimeError(f\"Torch = {v} too new!\")\nx = x.format(cuda.replace(\".\", \"\"), \"-ampere\" if is_ampere else \"\")\nprint(f'pip install --upgrade pip && pip install \"unsloth[{x}] @ git+https://github.com/unslothai/unsloth.git\"')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"pip install --upgrade pip && pip install \"unsloth[cu124-torch250] @ git+https://github.com/unslothai/unsloth.git\"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport pandas as pd\nfrom PIL import Image\nimport torch\nimport matplotlib.pyplot as plt\nfrom unsloth import FastVisionModel\nfrom datasets import Dataset, Image as HFImage\nfrom transformers import TextStreamer, BitsAndBytesConfig\nfrom unsloth import is_bf16_supported\nfrom unsloth.trainer import UnslothVisionDataCollator\nfrom trl import SFTTrainer, SFTConfig","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"TRAIN_CSV_PATH = \"/kaggle/input/train-captions/train_captions.csv\" # CSV with 'ID' and 'caption'\nTRAIN_IMAGE_DIR = \"/kaggle/input/image-clef2025/train/home/damm/clef/data/2025/splits/train/\"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"VAL_CSV_PATH = \"/kaggle/input/image-clef2025/valid_captions.csv\" # CSV with 'ID' and 'caption'\nVAL_IMAGE_DIR = \"/kaggle/input/image-clef2025/valid/home/damm/clef/data/2025/splits/valid/\"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"TEST_IMAGE_DIR = \"/kaggle/input/image-clef2025/test/test/\" # Folder with test images named by ID","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"OUTPUT_MODEL_DIR = \"/kaggle/working/finetuned_radiology_model\"\nSUBMISSION_FILE = \"/kaggle/working/submission.csv\"\nLOSS_PLOT_FILE = \"/kaggle/working/loss_plot.png\"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"MODEL_NAME = \"unsloth/Qwen2-VL-2B-Instruct-bnb-4bit\"\nMAX_SEQ_LENGTH = 2048\nLORA_R = 16\nLORA_ALPHA = 16\nTRAIN_BATCH_SIZE = 2\nGRAD_ACCUMULATION_STEPS = 4\nMAX_STEPS = 60 \nEVAL_STEPS = 10 \nLEARNING_RATE = 2e-4\nSEED = 3407","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"INSTRUCTION = \"You are an expert radiographer. Describe accurately what you see in this image.\"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def create_hf_dataset(csv_path, image_folder_path, dataset_type=\"train\"):\n    if not os.path.exists(csv_path):\n        print(f\"Warning: {dataset_type} CSV {csv_path} not found. Skipping {dataset_type} dataset.\")\n        return None\n    if not os.path.exists(image_folder_path):\n        print(f\"Warning: {dataset_type} image folder {image_folder_path} not found. Skipping {dataset_type} dataset.\")\n        return None\n    \n    try:\n        df = pd.read_csv(csv_path)\n    except Exception as e:\n        print(f\"Error reading {dataset_type} CSV {csv_path}: {e}. Skipping {dataset_type} dataset.\")\n        return None\n\n    data = {\"image_path_str\": [], \"Caption\": [], \"ID\": []} # Store paths in 'image_path_str'\n    print(f\"Loading {dataset_type} dataset from {csv_path} and {image_folder_path}...\")\n    for _, row in df.iterrows():\n        image_id = str(row[\"ID\"])\n        caption = str(row[\"Caption\"])\n        \n        found_image = False\n        extensions_to_check = ['.jpg', '.png', '.jpeg', '.bmp', '.tiff', '.dcm']\n        for ext in extensions_to_check:\n            potential_path = os.path.join(image_folder_path, image_id + ext)\n            if os.path.exists(potential_path):\n                data[\"image_path_str\"].append(potential_path)\n                data[\"Caption\"].append(caption)\n                data[\"ID\"].append(image_id)\n                found_image = True\n                break\n        \n        if not found_image:\n            print(f\"Warning: Image for ID {image_id} not found in {image_folder_path} with extensions {extensions_to_check}\")\n            \n    if not data[\"image_path_str\"]:\n        print(f\"Warning: No images found for {dataset_type} dataset. Skipping.\")\n        return None\n\n    # Create dataset with image paths in 'image_path_str'\n    hf_dataset = Dataset.from_dict({\"image_path_str\": data[\"image_path_str\"], \"Caption\": data[\"Caption\"], \"ID\": data[\"ID\"]})\n    \n    hf_dataset = hf_dataset.cast_column(\"image_path_str\", HFImage(decode=True))\n    \n    hf_dataset = hf_dataset.rename_column(\"image_path_str\", \"image\")\n\n    print(f\"Loaded and cast {len(hf_dataset)} samples for {dataset_type} dataset. Image column is now 'image'.\")\n    return hf_dataset","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_dataset_raw = create_hf_dataset(TRAIN_CSV_PATH, TRAIN_IMAGE_DIR, \"train\")\nval_dataset_raw = create_hf_dataset(VAL_CSV_PATH, VAL_IMAGE_DIR, \"validation\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"if train_dataset_raw:\n    print(\"\\nDebug: Checking train_dataset_raw immediately after cast_column (first 5 samples):\")\n    for i in range(min(5, len(train_dataset_raw))):\n        sample_check = train_dataset_raw[i]\n        print(f\"  ID: {sample_check.get('ID', 'N/A')}, Image Type: {type(sample_check.get('image'))}, Image is None: {sample_check.get('image') is None}\")\n        if not isinstance(sample_check.get('image'), Image.Image) and sample_check.get('image') is not None:\n            print(f\"  WARNING: Non-PIL Image object for ID {sample_check.get('ID', 'N/A')}: {sample_check.get('image')}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"if train_dataset_raw:\n    num_train_samples = len(train_dataset_raw)\n    num_samples_to_keep = int(0.5 * num_train_samples)\n    train_dataset_raw = train_dataset_raw.shuffle(seed=SEED) \n    train_dataset_raw = train_dataset_raw.select(range(num_samples_to_keep))\n    print(f\"Subsampled training dataset to {len(train_dataset_raw)} samples (50%).\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def is_image_valid(example):\n    image_data = example.get('image') \n    is_valid = image_data is not None and isinstance(image_data, Image.Image)\n    if not is_valid:\n        print(f\"FILTERING OUT: ID: {example.get('ID', 'Unknown ID')}, Image Type: {type(image_data)}, Image is None: {image_data is None}\")\n    return is_valid\n\nif train_dataset_raw:\n    initial_count_train = len(train_dataset_raw)\n    print(f\"Train dataset before filtering: {initial_count_train} samples.\")\n    train_dataset_raw = train_dataset_raw.filter(is_image_valid, load_from_cache_file=False)\n    filtered_count_train = len(train_dataset_raw)\n    print(f\"Train dataset after filtering: {filtered_count_train} samples. Filtered out: {initial_count_train - filtered_count_train}\")\n    if not train_dataset_raw or filtered_count_train == 0:\n        print(\"Training dataset is empty after filtering invalid images. Exiting.\")\n        exit()\n\nif train_dataset_raw:\n    print(\"\\nDebug: Checking train_dataset_raw after filter (first 5 samples):\")\n    for i in range(min(5, len(train_dataset_raw))):\n        sample_check = train_dataset_raw[i]\n        print(f\"  ID: {sample_check.get('ID', 'N/A')}, Image Type: {type(sample_check.get('image'))}, Image is None: {sample_check.get('image') is None}\")\n        if sample_check.get('image') is None or not isinstance(sample_check.get('image'), Image.Image):\n             print(f\"  CRITICAL WARNING: Invalid image for ID {sample_check.get('ID', 'N/A')} RETAINED after filter.\")\n\nif val_dataset_raw:\n    initial_count_val = len(val_dataset_raw)\n    print(f\"Validation dataset before filtering: {initial_count_val} samples.\")\n    val_dataset_raw = val_dataset_raw.filter(is_image_valid, load_from_cache_file=False)\n    filtered_count_val = len(val_dataset_raw)\n    print(f\"Validation dataset after filtering: {filtered_count_val} samples. Filtered out: {initial_count_val - filtered_count_val}\")\n    if not val_dataset_raw or filtered_count_val == 0:\n        print(\"Validation dataset is empty after filtering invalid images. Will proceed without validation.\")\n        val_dataset_raw = None","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def convert_to_conversation(sample):\n    if sample.get('image') is None or not isinstance(sample.get('image'), Image.Image):\n        print(f\"CRITICAL in convert_to_conversation: ID {sample.get('ID', 'Unknown ID')} has invalid image type: {type(sample.get('image'))}\")\n    return {\n        \"messages\": [\n            {\"role\": \"user\", \"content\": [\n                {\"type\": \"text\", \"text\": INSTRUCTION},\n                {\"type\": \"image\", \"image\": sample[\"image\"]} # sample[\"image\"] must be a PIL.Image object\n            ]},\n            {\"role\": \"assistant\", \"content\": [\n                {\"type\": \"text\", \"text\": sample[\"Caption\"]}\n            ]},\n        ]\n    }","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"if train_dataset_raw:\n    converted_train_dataset = train_dataset_raw.map(\n        convert_to_conversation, \n        batched=False,\n        remove_columns=train_dataset_raw.column_names, \n        load_from_cache_file=False \n    )\n    print(f\"Training dataset converted to conversation format. Num samples: {len(converted_train_dataset)}\")\nelse:\n    print(\"Training dataset could not be loaded or was empty after filtering. Exiting.\")\n    exit()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"converted_val_dataset = None\nif val_dataset_raw:\n    converted_val_dataset = val_dataset_raw.map(\n        convert_to_conversation,\n        batched=False,\n        remove_columns=val_dataset_raw.column_names,\n        load_from_cache_file=False\n    )\n    print(f\"Validation dataset converted to conversation format. Num samples: {len(converted_val_dataset)}\")\nelse:\n    print(\"No valid validation dataset available.\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model, tokenizer = FastVisionModel.from_pretrained(\n    MODEL_NAME,\n    load_in_4bit=True,\n    use_gradient_checkpointing=\"unsloth\",\n    max_seq_length=MAX_SEQ_LENGTH,\n)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model = FastVisionModel.get_peft_model(\n    model,\n    finetune_vision_layers=False,\n    finetune_language_layers=True,\n    finetune_attention_modules=True,\n    finetune_mlp_modules=True,\n    r=LORA_R,\n    lora_alpha=LORA_ALPHA,\n    lora_dropout=0,\n    bias=\"none\",\n    random_state=SEED,\n    use_rslora=False,\n    loftq_config=None,\n)\nprint(\"Model and tokenizer loaded with PEFT.\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"FastVisionModel.for_training(model)\n\ntrainer = SFTTrainer(\n    model=model,\n    tokenizer=tokenizer,\n    data_collator=UnslothVisionDataCollator(model, tokenizer), # Must use!\n    train_dataset=converted_train_dataset,\n    eval_dataset=converted_val_dataset,\n    args=SFTConfig(\n        per_device_train_batch_size=TRAIN_BATCH_SIZE,\n        gradient_accumulation_steps=GRAD_ACCUMULATION_STEPS,\n        warmup_steps=5,\n        max_steps=MAX_STEPS,\n        learning_rate=LEARNING_RATE,\n        fp16=not is_bf16_supported(),\n        bf16=is_bf16_supported(),\n        logging_steps=1, \n        optim=\"adamw_8bit\",\n        weight_decay=0.01,\n        lr_scheduler_type=\"linear\",\n        seed=SEED,\n        output_dir=OUTPUT_MODEL_DIR, \n        report_to=\"none\",\n        dataset_num_proc=4, \n        max_seq_length=MAX_SEQ_LENGTH,\n    ),\n)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(\"Starting training...\")\ntry:\n    trainer_stats = trainer.train()\n    print(\"Training finished.\")\nexcept Exception as e:\n    print(f\"Error during training: {e}\")\n    import traceback\n    traceback.print_exc()\n    print(\"Inspect the debug prints above, especially any 'FILTERING OUT' or 'CRITICAL' messages.\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# 5. Save Model\nprint(f\"Saving model to {OUTPUT_MODEL_DIR}...\")\ntrainer.save_model(OUTPUT_MODEL_DIR)\ntokenizer.save_pretrained(OUTPUT_MODEL_DIR)\nprint(\"Model and tokenizer saved.\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"log_history = trainer.state.log_history\ntrain_steps = []\ntrain_losses = []\neval_steps_log = []\neval_losses = []\n\nfor log_entry in log_history:\n    if 'loss' in log_entry and 'eval_loss' not in log_entry : # Training log\n        train_steps.append(log_entry['step'])\n        train_losses.append(log_entry['loss'])\n    if 'eval_loss' in log_entry: # Evaluation log\n        eval_steps_log.append(log_entry['step'])\n        eval_losses.append(log_entry['eval_loss'])\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"plt.figure(figsize=(12, 6))\nplt.plot(train_steps, train_losses, label='Training Loss', marker='o', linestyle='-')\nif eval_steps_log and eval_losses:\n    plt.plot(eval_steps_log, eval_losses, label='Validation Loss', marker='x', linestyle='--')\nplt.xlabel('Steps')\nplt.ylabel('Loss')\nplt.title('Training and Validation Loss Over Steps')\nplt.legend()\nplt.grid(True)\nplt.savefig(LOSS_PLOT_FILE)\nprint(f\"Loss plot saved to {LOSS_PLOT_FILE}\")\nplt.show()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(\"Starting inference on test set...\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"del model\ndel trainer\ntorch.cuda.empty_cache()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model, tokenizer = FastVisionModel.from_pretrained(\n    OUTPUT_MODEL_DIR,\n    load_in_4bit=True,\n    random_state=SEED,\n    max_seq_length=MAX_SEQ_LENGTH,\n)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"FastVisionModel.for_inference(model) \nprint(\"Fine-tuned model loaded for inference.\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"if not os.path.exists(TEST_IMAGE_DIR):\n    print(f\"Test image directory {TEST_IMAGE_DIR} not found. Skipping inference.\")\nelse:\n    test_image_files = []\n    for f_name in os.listdir(TEST_IMAGE_DIR):\n        full_path = os.path.join(TEST_IMAGE_DIR, f_name)\n        if os.path.isfile(full_path):\n            # Check for common image extensions\n            if f_name.lower().endswith(('.png', '.jpg', '.jpeg', '.bmp', '.tiff', '.dcm')):\n                test_image_files.append(full_path)\n    \n    if not test_image_files:\n        print(f\"No image files found in {TEST_IMAGE_DIR}. Skipping inference.\")\n    else:\n        print(f\"Found {len(test_image_files)} images for inference.\")\n        results = []\n        \n        for image_path in test_image_files:\n            image_id = os.path.splitext(os.path.basename(image_path))[0]\n            try:\n                pil_image = Image.open(image_path).convert(\"RGB\") # Ensure RGB\n            except Exception as e:\n                print(f\"Could not load test image {image_path}: {e}\")\n                results.append({\"ID\": image_id, \"Caption\": f\"Error loading image: {e}\"})\n                continue\n\n            messages_inference = [\n                {\"role\": \"user\", \"content\": [\n                    {\"type\": \"image\"}, # Placeholder for the image\n                    {\"type\": \"text\", \"text\": INSTRUCTION}\n                ]}\n            ]\n            \n            # Prepare text prompt using chat template\n            text_prompt = tokenizer.apply_chat_template(\n                messages_inference,\n                tokenize=False, # Get the string prompt\n                add_generation_prompt=True\n            )\n\n            inputs = tokenizer(\n                images=pil_image,\n                text=text_prompt,\n                return_tensors=\"pt\",\n                add_special_tokens=False, \n            ).to(\"cuda\")\n\n            text_streamer = TextStreamer(tokenizer, skip_prompt=True)\n            print(f\"Generating caption for {image_id}...\")\n            with torch.no_grad():\n                outputs = model.generate(\n                    **inputs,\n                    streamer=text_streamer, \n                    max_new_tokens=512,   \n                    use_cache=True,\n                    temperature=0.7,       \n                    min_p=0.1\n                )\n            \n            input_ids_len = inputs.input_ids.shape[1]\n            generated_ids = outputs[0][input_ids_len:]\n            predicted_caption = tokenizer.decode(generated_ids, skip_special_tokens=True).strip()\n            \n            print(f\"Predicted for {image_id}: {predicted_caption}\")\n            results.append({\"ID\": image_id, \"Caption\": predicted_caption})\n            pil_image.close()\n\n        submission_df = pd.DataFrame(results)\n        submission_df.to_csv(SUBMISSION_FILE, index=False)\n        print(f\"Submission file created: {SUBMISSION_FILE}\")\n\nprint(\"Script finished.\")","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}